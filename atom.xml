<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Yaman Parasher</title>
 <link href="https://martinjedwabny.github.io/atom.xml" rel="self"/>
 <link href="https://martinjedwabny.github.io/"/>
 <updated>2020-09-18T19:27:50+02:00</updated>
 <id>https://parasheryaman19.github.io</id>
 <author>
   <name>Yaman Parasher</name>
   <email></email>
 </author>

 
 <entry>
   <title>Statistical relational learning</title>
   <link href="https://martinjedwabny.github.io/2019/11/25/srl/"/>
   <updated>2019-11-25T00:00:00+01:00</updated>
   <id>https://martinjedwabny.github.io/2019/11/25/srl</id>
   <content type="html">&lt;!-- What is srl --&gt;
&lt;p&gt;Statistical relational learning &lt;a href=&quot;#koller2007introduction&quot;&gt;(Koller et al., 2007)&lt;/a&gt; is a branch of artificial intelligence (AI) devoted to integrate research in probability theory, statistics, logics and relational learning.
Its main purpose is to develop learning models that handle uncertain information extracted from real world scenarios, and produce structured representations that describe objects, attributes and their relations.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Markov Logic Networks</title>
   <link href="https://martinjedwabny.github.io/2019/10/23/markov/"/>
   <updated>2019-10-23T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/23/markov</id>
   <content type="html">&lt;p&gt;Markov Logic Networks &lt;a href=&quot;#markov-2006&quot;&gt;(Richardson &amp;amp; Domingos, 2006)&lt;/a&gt;, a.k.a. MLN, is a representation and query answering framework that combines probability and first-order logic.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Bayesian and Markov networks</title>
   <link href="https://martinjedwabny.github.io/2019/10/23/bay-markov/"/>
   <updated>2019-10-23T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/23/bay-markov</id>
   <content type="html">&lt;p&gt;In this article I will briefly explain and sketch the ideas behind Bayesian and Markov networks. These networks model joint probability distributions through graphs.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Implicitly Learning to Reason in First-Order Logic</title>
   <link href="https://martinjedwabny.github.io/2019/10/21/implicit-learning/"/>
   <updated>2019-10-21T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/21/implicit-learning</id>
   <content type="html">&lt;p&gt;This paper &lt;a href=&quot;#implicit-learning-2019&quot;&gt;(Belle &amp;amp; Juba, 2019)&lt;/a&gt; considers the problem of answering queries about infinitary first order logic (infinite but countable amount of constants) formulas based on background knowledge partially represented explicitly as other formulas, and partially represented as examples independently drawn from a fixed probability distribution.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>How Powerful are Graph Neural Networks?</title>
   <link href="https://martinjedwabny.github.io/2019/10/21/gnns/"/>
   <updated>2019-10-21T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/21/gnns</id>
   <content type="html">&lt;p&gt;Graph neural networks (GNNs) is a framework that allows to learn representations in a graph. This paper &lt;a href=&quot;#gnn-xu-2019&quot;&gt;(Xu et al., 2018)&lt;/a&gt; discusses previous architectures, analyses their power of representation and presents their own GNN which is maximally powerful under certain constraints.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Tensorlog</title>
   <link href="https://martinjedwabny.github.io/2019/10/15/tensorlog/"/>
   <updated>2019-10-15T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/15/tensorlog</id>
   <content type="html">&lt;p&gt;TensorLog &lt;a href=&quot;#cohenYM17&quot;&gt;(Cohen et al., 2017)&lt;/a&gt; is an implementation of probabilistic knowledge bases that (i) leverages deep learning to answer queries efficiently, by (ii) restricting the first order logic language to a class called p-tree-DKG, and (iii) learns weights on knowledge base facts.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Logic Tensor Networks</title>
   <link href="https://martinjedwabny.github.io/2019/10/08/ltn/"/>
   <updated>2019-10-08T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/08/ltn</id>
   <content type="html">&lt;!-- Intro --&gt;

&lt;p&gt;Logic Tensor Networks &lt;a href=&quot;#serafini2016logic&quot;&gt;(Serafini &amp;amp; Garcez, 2016)&lt;/a&gt; is a framework for learning and reasoning combining neural-network capabilities with knowledge base data representation structures using first-order logic.&lt;/p&gt;

&lt;!-- Problem --&gt;

&lt;p&gt;They address the problem of (i) query answering in the unit interval [0,1] (also known as fuzzy or real logic), as well as, (ii) subsymbolic (vector) representation finding of constant symbols, function symbols and predicates.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Neural Theorem Provers</title>
   <link href="https://martinjedwabny.github.io/2019/10/07/end-to-end/"/>
   <updated>2019-10-07T00:00:00+02:00</updated>
   <id>https://martinjedwabny.github.io/2019/10/07/end-to-end</id>
   <content type="html">&lt;p&gt;This paper &lt;a href=&quot;#rocktaschel2017end&quot;&gt;(Rockt√§schel &amp;amp; Riedel, 2017)&lt;/a&gt; presents Neural Theorem Provers (NTP), a framework for fuzzy/real logic i.e. [0,1]-query answering with backwards chaining and subsymbolic representation learning.&lt;/p&gt;

</content>
 </entry>
 

</feed>
